---
layout: post
title: "validating AI workflows with no-code mvps"
description: "a simple approach to proving a problem exists before investing in engineering."
---

i was on a call recently with a group of subject matter experts who want to build an AI app in their field.

they asked me a good question… what is the best way to validate their problem space before investing in engineering?

after building multiple full-stack AI apps and backends, i’ve learned how important it is to validate the problem before committing real time and money. here is my preferred, simple way to build a no-code minimum viable product (mvp) to quickly validate a workflow:

## step 1: build custom gpts to prove out your core AI workflow

for many AI native apps, getting the “context” right is critical. the context is what you send the llm above and beyond a user prompt.

custom gpts are a simple, no-code tool that gives you fine-grained control over context. build versions for each workflow and iterate until it works.

## step 2: show the custom gpts to real (potential) users

walk them through the workflow live. even if it’s not perfect, you will get valuable discussion that helps confirm whether the workflow solves a real pain.

this approach has limitations — custom gpts don’t connect to backends, so they can’t personalize deeply — but they are more than enough for many no-code mvps. and as a bonus, they become great reference material if you later team up with a technical partner.

does this resonate? how are you validating AI apps and workflows before investing in development?
